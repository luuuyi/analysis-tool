1.设计了一个4层的神经网络，除了输入层和输出层，总共包含两个隐藏层，由于训练数据比较少，每个隐藏层设计为只包含3个神经元，每个隐藏层的神经元之后与一个激活函数单元相连接，使用的激活函数为sigmoid函数；
最终的设计结构如下，同时包含每个层最终的参数结果，loss最终维持在0.1667，因为第一，二个样本特征相同，但是输出结果不同，使得loss降为最低的方式就是使得该特征的样本最后的输出为两个label值的均值：

3.硕士期间和实习期间，我的主要研究方向是基于深度学习的物体检测，大部分时间其实是在跟各种物体检测的方法比如Faster RCNN，SSD等打交道，由于项目需要，最终的检测模型需要在嵌入式端使用，所以对于模型的大小还有模型做一次前向运算的耗时是有严格要求的，所以在训练DL模型的时候最大的挑战便来自于模型压缩和裁剪部分；
因为嵌入式端的运算能力有限，所以一开始对模型压缩主要目标是，降低模型的参数量（caffemodel文件的大小）和运算量；解决方法为，首先从模型上入手，对于选用的神经网络，一定不能包含全连接层，全连接层带来的是大量的待训练参数和时间消耗，所以将之前主流网络的全连接层使用卷积层来替代，实现全卷积网络；同时在卷积核的选用上，最大的卷积核为3x3的结构，并且尽量缩减3x3的卷积核数量，大量使用1x1卷积核，在降低模型参数数量的同时，使用1x1卷积核可以增强模型的线性，非线性表达能力同时也使用类bottleneck结构对feature map进行灵活的通道维度升维降维；
同时对于训练出来的模型，借助于song han的深度压缩那篇论文，对训练出来的模型参数进行舍弃+微调的方式进行模型的剪枝操作；最终大大的提升了模型在嵌入式端的运算能力；

4.我不认可深度学习是黑盒子的理论，不管是从程序设计还是数学理论的角度来看，深度学习都是基于一种规则去进行运算和优化的；
从深度学习比如卷积神经网络的前向计算来看，所依赖的运算，不管是卷积操作，pooling，激活函数层，全连接层，直到最后的输出层，每一层的计算都经过了数学理论的推导和框架的单元测试，每一条指令每一个公式都是对用户透明的，并不存在黑盒子的外在特征；
在神经网络的loss计算，优化方法和导数反向链式传播的过程中，也是有权威的数学理论和鲁棒且透明的计算框架去支撑的；回到黑盒子问题上，普遍能听到的说法就是DL的表现结果特别好，但是却不能得知其中的具体联系和计算原理，我觉得首先应该一分为二的来看，从人类的角度，由于大脑的限制，我们对于高度非线性的函数和高纬度的运算是无法直观感受到的，也就是不敏感的，但是只要理论基础没错，对于计算机，特别是现在多核GPU的计算能力，这些都是能被其快速正确的计算出来的；
同时，以图像应用，比如说分类来说，不从计算角度来看，从感性的方向来看，DL对于图像分类的任务，其实依赖的就是基于大规模数据的数据分布的学习，比如对于一个物体，一只猫，我们有大量的训练样本，DL在无数次迭代中学习的是一个尽量泛化的高度非线性模型去拟合包含这类对象（猫）的多维数据（图片），本质上来看就是一个拟合，回归的任务，通过反向传导结合优化方法，不断的更新这个模型的参数。所以，对于DL来说，也会出现模型“不认识”的情况，其实就是新来的图像的数据分布并不是DL模型训练的时候所“看见”过的；
所以，不管是从理性还是感性的角度来看，DL都是一个可以推敲的方法，我并不认可黑盒子一说。